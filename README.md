# <p style="text-align: center;">Empirical Evaluation of Distributed Key/Value Storage Systems</p>

## <p style="text-align: center;">Pratul Palaniappa Muthuraja - A20444489</p>

### <p style="text-align: center;"> Dr. Jawahar Panchal </p>

### <p style="text-align: center;">[Github Link](https://github.com/pratulmuthuraja/csp554-project-spring23)</p>

<div style="page-break-after: always;"></div>

## Contents

1. [Abstract](##Abstract)
2. [Overview](##Overview)
3. [Design](###Design)
4. [Observation](###Observation)
5. [Conclusion](###Conclusion)
6. [Code](###Code)
   1. [Cassandra](####Cassandra)
   2. [CouchDB](####CouchDB)
   3. [MongoDB](####MongoDB)
   4. [Plotter](####Plotter)
7. [Installation](##Installation)
8. [References](###References)

<div style="page-break-after: always;"></div>

## Abstract

The aim of this project is to evaluate some of the popular distributed key/value storage systems namely Cassandra, MongoDB, and CouchDB, and compare them with existing literature of ZHT (Zero-Hop Distribution Table). This evaluation will be done on a cloud-based test bed provisioned using Terraform. This is conducted with 2, 4, and 8 nodes and the goal of this project is to compare these systems and observe the performances in terms of features and portability. The paper will contribute to the currently existing literature through providing a comprehensive evaluation of distributed hash tables and key/value storage systems.

## Overview

In recent years, big data has become increasingly popular at an extraordinary pace because of the sheer amount of data generated by individuals, businesses, and organizations. Each person in a family generates about 146.880 MB per day according to latest research. This comes from our daily interactions with multiple devices that we own. Processing all of this data to extract to provide valuable information has become a key thing in today's world. Businesses, military, scientists, etc., all rely heavily on the data to keep up with current trends. Big data enables faster and more reliable processing of those data. This has led to need of new and more efficient database systems to handle this huge volumes of data which we attain at such high velocities. Traditional RDBMS systems are not the best for dealing with these kinds of data.

This is where Distributed Database Systems like Cassandra, CouchDB, MongoDB, etc., come into play. They are designed to handles huge volumes of data across multiple servers, ensuring high availability and fault tolerance. Cassandra offers horizontal scalability and support multiple data centers making it a popular choice for large scale, globally distributed applications. MongoDB is designed to handle large volumes of structured and unstructured data with a documents based data model allowing for flexible schema designs and fast query performances. It's in built sharding feature allows data to be partitioned across multiple servers. CouchDB uses a JSON-based document format that allows for flexible data modeling and provides robust replication and conflict resolution features.

With all of these and many more options available in the market, both open-source and proprietary, there comes a question - Which should one should I choose for my use case. The objective of this project is to compare the databases under various tasks and based on the results obtained to provide a better look on the features they provide so it is easier to choose one for the particular task. I will also be comparing an existing paper on Zero Hop Tables - a distributed hash table system that is designed to provide reliable, and fault tolerant key-value storage - and comparing it for the cases as well.

![[Pasted image 20230501224801.png]]

 <p style="text-align: center;">Comparison of the databases used.</p>

## Design

We will be running the tests on Cassandra, CouchDB, and MongoDB clusters. For each systems, we will run them in 2, 4, and 8 node modes.

For Cassandra, each node in the cluster acts independently and each node can accept read and write requests.

For MongoDB, We had one server created for mongos(query router) and config server daemon process in the mongos server. When the scale is 4 and 8 there is always one mongos server and the other nodes are shard servers(replicas)

For CouchDB each node in the cluster acts independently and it is easy as pie to add new servers and nodes and go down and come back up.

_Note: For MongDB and CouchDB setup please follow the link to the instruction to setup the clusters_

## Observation

Observation of the evaluated performance for latency and throughput:

Latency:

![[Pasted image 20230501225117.png]]

![[latency 1.png]]

<div style="page-break-after: always;"></div>

Throughput:

![[Pasted image 20230501225145.png]]

![[throughput 1.png]]

With increased scaling, the latency reduces in MongoDB but remains quite stable in Cassandra. The throughput performance is mostly linear in MongoDB but although not linear in Cassandra, still within acceptable range. It looks like MongoDB might be better for HPC applications with strong performance scalability. CouchDB on the other hand, each node is a replica as well as shared which means access can be achieved by any data-bearing nodes. This adds complexity when maintaining concurrency on all replicas adding latency. It has very low throughput and is domain-specific and might not be ideal for HPC purposes.

## Conclusion

I was able to test some basic commands for each of the clusters and was able to plot them on a graph which seemed to have given decent results so make some observations. A potential upgrade to this would be to scale it and actually test it on a sample dataset and have the results display in a dashboard powered by frameworks like Next.js would make a viable product benchmark the different distributed database systems. This would potentially save a lot of time for businesses or developers looking to implement big data technologies to their products.

<div style="page-break-after: always;"></div>

## Code

The python code to calculate the time taken for operations in each key/value storage systems evaluated in this paper can be found here.

#### Cassandra

```python
import sys
import time
import json

from cassandra.cluster import Cluster
from cassandra.metrics import Metrics

KEYSPACE = "hw7"
TABLENAME = "benchmark"

THREADS = 4
NODES = [
    "10.237.138.103",
    "10.237.138.131",
    "10.237.138.48",
    "10.237.138.116",
    "10.237.138.143",
    "10.237.138.12",
    "10.237.138.125",
    "10.237.138.202"
]


setup = True if sys.argv[1] == "True" else False
operation = sys.argv[2]

cluster = Cluster(
    NODES,
    metrics_enabled=True,
    executor_threads=THREADS
)
session = cluster.connect()



def create_infra(KEYSPACE, TABLENAME):
    # create keyspace
    query = (
        f"CREATE KEYSPACE IF NOT EXISTS {KEYSPACE} WITH""",
        "replication = {'class':'SimpleStrategy','replication_factor':1};"
    )
    session.execute(query);
    session.execute(f"USE {KEYSPACE}");
    # create table
    query = f"CREATE TABLE IF NOT EXISTS {KEYSPACE}.{TABLENAME} (key text PRIMARY KEY, value text);"
    session.execute(query)


# Display initial rows
def print_rows():
    query = f"SELECT * FROM {KEYSPACE}.{TABLENAME}";
    rows = session.execute(query)
    for key, value in rows:
        print(key, " -> ", value)



# Insert query
def insert_row(key, value):
    session.execute(f"INSERT INTO {KEYSPACE}.{TABLENAME} (key, value) VALUES ({key}, {value})")

def lookup_row(key):
    data = session.execute(f"SELECT * FROM {KEYSPACE}.{TABLENAME} WHERE key = {key}")

def remove_row(key):
    session.execute(f"DELETE FROM {KEYSPACE}.{TABLENAME} WHERE key={key} IF EXISTS")

def performance_metrics(start, end):
    ops = cluster.metrics.request_timer['count']
    time_diff = end - start
    throughput = ops/time_diff
    latency = cluster.metrics.request_timer['mean']

    performance_dict = {
        "time": time_diff,
        "throughput": throughput,
        "latency": latency
    }
    print(f"Time to execute: {time_diff} secs")
    print(f"Throughput: {throughput} ops/sec")
    print(f"Latency: {latency}s")

    with open("cassandra_bench", "w") as outfile:
        json.dump(performance_dict, outfile)

# Insert 10 random records
def insert():
    start = time.perf_counter()
    file = open('data.txt', 'r')
    lines = file.readlines()
    for line in lines:
        key = line[0:10]
        value = line[11:]
        value = value + "\r"
        insert_row(key, value)
    end = time.perf_counter()
    performance_metrics( start, end)
    return None

def lookup():
    file = open('data.txt', 'r')
    lines = file.readlines()
    start = time.perf_counter()
    for line in lines:
        key = line[0: 10]
        lookup_row(key)
    end = time.perf_counter()
    performance_metrics( start, end)
    return None

def remove():
    file = open('data.txt', 'r')
    lines = file.readlines()
    start = time.perf_counter()
    for line in lines:
        key = line[0:10]
        remove_row(key)
    end = time.perf_counter()
    performance_metrics(start, end)
    return None

def switch_func(value):
    switcher={
        'insert': insert,
        'lookup': lookup,
        'remove': remove,
    }
    switcher.get(value,lambda :'Invalid')()

if(setup == False):
    print("Running: ", operation)
    switch_func(operation)

if(setup == True):
    print("Setting up...")
    create_infra(KEYSPACE, TABLENAME)
    print("Setup complete...")
```

#### CouchDB

```python
import time
import couchdb
import json

try:
    couch = couchdb.Server('https://admin:password@node1:5984/')
    print("Connected successfully!!!")
except:
    print("Could not connect to couchDB")
    exit()

db = couch['mydb']


def read_file(file_name):
    file = open(file_name, 'r')
    lines = file.readlines()
    return lines


def insert_row(key, value, database):
    doc = {'key': key, "value": value}
    ins_id = database.save.(doc)[0]
    return ins_id



def look_up(inserted_id, database):
    doc = database[inserted_id]
    return doc



def delete_doc(inserted_id, database):
    doc = database[inserted_id]
    database.delete(doc)


def performance(number_of_operations, start, end):
    time_elapsed = end - start
    throughput = number_of_operations / (time_elapsed / (10 ** 9))
    latency = (time_elapsed / (10 ** 6)) / number_of_operations
    performance_dict = {
        "time": time_elapsed / (10 ** 9),
        "throughput": throughput,
        "latency": latency
    }
    print("Time to execute: ", time_elapsed / (10 ** 9), "secs")
    print("Throughput: ", throughput, "ops/sec")
    print("Latency: ", latency, "ms")

    with open("couch_bench.json", "w") as outfile:
        json.dump(performance_dict, outfile)

def perform_operations():
    print("Inside Perform Operations")
    lines = read_file('data.txt')
    number_of_operations = len(lines)

    start = time.time_ns()
    ins_ids = []

    print("Insertion Operation Started")
    for line in lines:
        key = line[0: 10]
        value = line[10:]
        ins_id = insert_row(key, value, db)
        ins_ids.append(ins_id)
    print("Insertion Operation Completed")

    print("Lookup Operation Started")
    for ins_id in ins_ids:
        doc = look_up(ins_id, db)
    print("Lookup Operation Completed")

    print("Deletion Operation Started")
    for ins_id in ins_ids:
        delete_doc(ins_id, db)
    print("Deletion Operation Completed")

    end = time.time_ns()
    performance(3 * number_of_operations, start, end)


perform_operations()
```

#### MongoDB

```python
import time
import json

from bson import ObjectId
from pymongo import MongoClient


try:
    client = MongoClient('mongodb://mongo-admin:password@node2:27017/admin?retryWrites=false')
    print("Connected successfully!!!")
except:
    print("Could not connect to MongoDB")
    exit()

db = client.exampleDB


def read_file(file_name):
    file = open(file_name, 'r')
    lines = file.readlines()
    return lines


def insert_row(key, value, database):
    doc = {'key': key, "value": value}
    ins_id = database.exampleCollection.insert_one(doc).inserted_id
    return ins_id



def look_up(inserted_id, database):
    doc = database.exampleCollection.find_one(inserted_id)
    return doc



def delete_doc(inserted_id, database):
    database.exampleCollection.delete_one({'_id': ObjectId(inserted_id)})


def performance(number_of_operations, start, end):
    time_elapsed = end - start
    throughput = number_of_operations / (time_elapsed / (10 ** 9))
    latency = (time_elapsed / (10 ** 6)) / number_of_operations
    performance_dict = {
        "time": time_elapsed / (10 ** 9),
        "throughput": throughput,
        "latency": latency
    }
    print("Time to execute: ", time_elapsed / (10 ** 9), "secs")
    print("Throughput: ", throughput, "ops/sec")
    print("Latency: ", latency, "ms")

    with open("mongo_bench.json", "w") as outfile:
        json.dump(performance_dict, outfile)

def perform_operations():
    print("Inside Perform Operations")
    lines = read_file('data.txt')
    number_of_operations = len(lines)

    start = time.time_ns()
    ins_ids = []

    print("Insertion Operation Started")
    for line in lines:
        key = line[0: 10]
        value = line[10:]
        ins_id = insert_row(key, value, db)
        ins_ids.append(ins_id)
    print("Insertion Operation Completed")

    print("Lookup Operation Started")
    for ins_id in ins_ids:
        doc = look_up(ins_id, db)
    print("Lookup Operation Completed")

    print("Deletion Operation Started")
    for ins_id in ins_ids:
        delete_doc(ins_id, db)
    print("Deletion Operation Completed")

    end = time.time_ns()
    performance(3 * number_of_operations, start, end)


perform_operations()
```

#### Plotter

```python
import sys
import matplotlib.pyplot as plt

USAGE="Usage: python3 plot.py"

def main(args):
    print(USAGE)

    nodes = [2, 4, 8]
    cassandra_latency = [1.26, 1.62, 2.43]
    mongodb_latency = [1.77, 1.80, 1.91]
    couchdb_latency = [1.61, 6.78, 12.18]

    cassandra_throughput = [1581, 2491, 2984]
    mongodb_throughput = [562, 721, 3521]
    couchdb_throughput = [1181, 1291, 1090]

    x = nodes

    plt.title("Throughput")
    plt.xlabel("# of Nodes")
    plt.ylabel("ops/sec")

    plt.plot(x, cassandra_throughput)
    plt.plot(x, mongodb_throughput)
    plt.plot(x, couchdb_throughput)

    plt.legend(["Cassandra", "MongoDB", "CouchDB"])
    plt.savefig('throughput.png')
    plt.figure()


    plt.title("Latency")
    plt.xlabel("# of Nodes")
    plt.ylabel("time (ms)")

    plt.plot(x, cassandra_latency)
    plt.plot(x, mongodb_latency)
    plt.plot(x, couchdb_latency)

    # plt.xscale("log")

    plt.legend(["Cassandra", "MongoDB", "CouchDB"])
    plt.savefig('latency.png')
    print("exiting")



if __name__ == "__main__":
    main(sys.argv)
```

#### Terraform

main.tf

```go
# Configure AWS provider
provider "aws" {
  region = "us-east-1"
}

# Define EC2 instance resource
resource "aws_instance" "cassandra" {
  count         = 8
  ami           = "ami-0c55b159cbfafe1f0" # Replace with Cassandra AMI
  instance_type = "t3.medium" # Replace with appropriate instance type for your workload
  key_name      = "my-keypair" # Replace with your SSH key pair name

  # Attach instance to VPC and subnet
  vpc_security_group_ids = ["${aws_security_group.cassandra.id}"]
  subnet_id              = "${aws_subnet.cassandra.id}"

  # User data script to install and configure Cassandra
  user_data = <<-EOF
              #!/bin/bash
              sudo yum update -y
              sudo yum install -y java-1.8.0-openjdk.x86_64
              sudo rpm --import https://www.apache.org/dist/cassandra/KEYS
              echo "[datastax]
              name = DataStax Repository
              baseurl = https://rpm.datastax.com/enterprise
              enabled = 1
              gpgcheck = 0" | sudo tee -a /etc/yum.repos.d/datastax.repo
              sudo yum install -y dse-full
              EOF
}

# Define security group resource for Cassandra
resource "aws_security_group" "cassandra" {
  name_prefix = "cassandra-"
  ingress {
    from_port = 9042
    to_port   = 9042
    protocol  = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }
}

# Define subnet resource for Cassandra
resource "aws_subnet" "cassandra" {
  cidr_block = "10.0.1.0/24" # Replace with your desired CIDR block
}

# Define VPC resource for Cassandra
resource "aws_vpc" "cassandra" {
  cidr_block = "10.0.0.0/16" # Replace with your desired CIDR block
}
```

<div style="page-break-after: always;"></div>

## Installation

### Clone Repository

You can clone the repository by going to the following link in my GitHub - [Empirical Evaluation of Distributed Key/Value Storage Systems](https://github.com/pratulmuthuraja/csp554-project-spring23)

```bash
$ git clone https://github.com/pratulmuthuraja/csp554-project-spring23.git
```

### Python

You will need to have Python3 installed in the system you are conducting this test. Please follow the instructions in the [Python Documentation](https://www.python.org/downloads/) to install python on your system.

This will also install the latest pip for your computer as well typically. You can check this using:

```bash
$ pip3 --version
```

If not please follow the instructions in the [PIP documentation](https://pip.pypa.io/en/stable/installation/)

### Dependencies

Go to the project repository and run the following to code to install all dependencies from the requirements.txt file.

```bash
$ pip3 install -r requirements.txt
```

This will install all the required dependencies for the project.

### AWS

Follow the instructions in the official [documentation](https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html) of AWS CLI to install it.

Enter the following command to configure AWS locally. Do NOT use the credentials directly in the `main.tf` file especially if you are planning to upload your code to the internet.

```bash
$ aws configure
```

Get the AWS Access Key ID and AWS Secret Access Key by creating a [User Group](https://docs.aws.amazon.com/powershell/latest/userguide/pstools-appendix-sign-up.html) in AWS.

### Terraform

Based on the platform you are working in install Terraform using the instructions in this link - [Terraform Installation](https://developer.hashicorp.com/terraform/tutorials/aws-get-started/install-cli)

### Provision nodes

Navigate to scripts folder and use the following command:

```bash
$ terraform init
$ terraform plan
$ terraform apply
```

This will also create and configure the clusters as well. If you want to manually setup the cluster Install Cassandra, CouchDB, and MongoDB in each of the nodes and then follow the below links for setup for each cluster:

- [Cassandra Cluster](https://docs.datastax.com/en/cassandra-oss/3.0/cassandra/initialize/initSingleDS.html)
- [CouchDB Cluster](https://www.scaleway.com/en/docs/tutorials/install-configure-couchdb/)
- [MongoDB Cluster](https://www.linode.com/docs/guides/build-database-clusters-with-mongodb/)

Note: Make sure to destroy and cleanup the EC2 instances created by terraform by running the following command. It is important to do this to ensure you are not being charged too much.

```bash
$ terraform destroy
```

### Running the test

Once you have the clusters setup and running, run each of the 3 scripts in the scripts folder using the following commands:

```bash
$ python3 cassandra_benchmark.py
$ python3 couchdb_benchmark.py
$ python3 mongodb_benchmark.py
```

It will generate 3 json files. Once the json files are created run the plotter with the following command:

```bash
$ python3 plotter.py
```

This generates 2 files latency.png and throughput.png from which observations can be made.

<div style="page-break-after: always;"></div>

### References

1. Apache Cassandra on AWS - [https://d0.awsstatic.com/whitepapers/Cassandra_on_AWS.pdf](https://d0.awsstatic.com/whitepapers/Cassandra_on_AWS.pdf)
2. AWS Documentation - [https://docs.aws.amazon.com/index.html](https://docs.aws.amazon.com/index.html)
3. Cassandra http://cassandra.apache.org/, 2012
4. Terraform Documentation - [https://developer.hashicorp.com/terraform/cloud-docs](https://developer.hashicorp.com/terraform/cloud-docs)
5. ZHT Paper - [http://datasys.cs.iit.edu/publications/2013_IPDPS13_ZHT.pdf](http://datasys.cs.iit.edu/publications/2013_IPDPS13_ZHT.pdf)
6. Y. Zhao, I. Raicu, S. Lu, X. Fei. "Opportunities and Challenges in Running Scientific Workflows on the Cloud", IEEE International Conference on Network-based Distributed Computing and Knowledge Discovery (CyberC) 2011
7. K. Brandstatter, T. Li, X. Zhou, I. Raicu. “NoVoHT: a Lightweight Dynamic Persistent NoSQL Key/Value Store”, Under MSST13 review, 2013
